{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial.distance import pdist\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"/Users/mac/Desktop/gnn/archive/consolidated_train.csv\", header=None )\n",
    "df = df.iloc[1:, :]\n",
    "df.columns = ['frame_num', 'identity_num', 'bbox_top', 'bbox_left', 'bbox_width',\n",
    "              'bbox_height', 'object_category', 'occlusion', 'truncation', 'object_pose']\n",
    "\n",
    "# Data Preprocessing\n",
    "le = LabelEncoder()\n",
    "df['object_pose'] = le.fit_transform(df['object_pose'])\n",
    "df[['bbox_top', 'bbox_left', 'bbox_width', 'bbox_height']] = df[['bbox_top', 'bbox_left', 'bbox_width', 'bbox_height']].apply(pd.to_numeric, errors='coerce')\n",
    "df = df.dropna(subset=['bbox_top', 'bbox_left', 'bbox_width', 'bbox_height'])\n",
    "# Ensure bounding box dimensions are positive\n",
    "df = df[(df['bbox_width'] > 0) & (df['bbox_height'] > 0)]\n",
    "# Checking class distribution\n",
    "print(df['object_pose'].value_counts())\n",
    "df['object_pose'] = pd.to_numeric(df['object_pose'], errors='coerce', downcast='integer')\n",
    "\n",
    "# You might decide on further actions based on the output\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['object_pose'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, stratify=train_df['object_pose'], random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "# Statistical summary\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_for_frame(df_frame):\n",
    "    # Use a copy to avoid SettingWithCopyWarning\n",
    "    df_frame = df_frame.copy()\n",
    "\n",
    "    nodes = df_frame[['bbox_top', 'bbox_left', 'bbox_width', 'bbox_height']].values\n",
    "    centers = np.column_stack([\n",
    "        df_frame['bbox_left'] + df_frame['bbox_width'] / 2, \n",
    "        df_frame['bbox_top'] + df_frame['bbox_height'] / 2\n",
    "    ])\n",
    "    kdtree = KDTree(centers)\n",
    "    distances = pdist(centers, metric='euclidean')\n",
    "    threshold = np.percentile(distances, q=90)  \n",
    "    \n",
    "    indices_list = kdtree.query_radius(centers, r=threshold)\n",
    "    \n",
    "    spatial_edges = set()\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        edges = [(i, j) for j in indices if i != j]  \n",
    "        spatial_edges.update(edges)\n",
    "    edge_index = torch.tensor(list(spatial_edges), dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(nodes, dtype=torch.float)\n",
    "\n",
    "    # Adjusted lines to handle SettingWithCopyWarning\n",
    "    df_frame.loc[:, 'object_pose'] = pd.to_numeric(df_frame['object_pose'], errors='coerce', downcast='integer')\n",
    "    df_frame.loc[:, 'object_pose'].fillna(-1, inplace=True)\n",
    "    \n",
    "    y = torch.tensor(df_frame['object_pose'].astype(int).values, dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "def construct_temporal_graph_for_frame(df_frame, df_next_frame):\n",
    "    nodes = df_frame[['bbox_top', 'bbox_left', 'bbox_width', 'bbox_height']].values\n",
    "    common_ids = set(df_frame['identity_num']).intersection(df_next_frame['identity_num'])\n",
    "    current_indices = np.where(df_frame['identity_num'].isin(common_ids))[0]\n",
    "    next_indices = np.where(df_next_frame['identity_num'].isin(common_ids))[0] + len(df_frame)\n",
    "    \n",
    "    temporal_edges = list(zip(current_indices, next_indices))\n",
    "    \n",
    "    edge_index = torch.tensor(temporal_edges, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(nodes, dtype=torch.float)\n",
    "    y = torch.tensor(df_frame['object_pose'].values, dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, num_classes)  \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.4, training=self.training)  \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # 3. Loss Function\n",
    "def detection_loss(pred_class_scores, true_class_labels):\n",
    "    classification_loss = F.cross_entropy(pred_class_scores, true_class_labels)\n",
    "    return classification_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece170e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(folder='results'):\n",
    "    \"\"\"Generate graphs one by one from saved chunks.\"\"\"\n",
    "    saved_files = [f for f in os.listdir(folder) if f.startswith('result_batch_')]\n",
    "    for file_name in saved_files:\n",
    "        yield torch.load(os.path.join(folder, file_name))\n",
    "# Training and Validation Functions\n",
    "def train_epoch(loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0  \n",
    "    total = 0\n",
    "    for data in loader:  # Use provided loader instead of fixed train_loader\n",
    "        data, target = data.to(device), data.y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.x, data.edge_index)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    average_loss = total_loss / len(loader)\n",
    "    train_accuracy = correct / total  \n",
    "    val_accuracy , val_f1 = validate(val_loader, model, device, criterion)  # Ensure validate function is defined with proper parameters\n",
    "    \n",
    "    return average_loss, train_accuracy\n",
    "\n",
    "def validate(loader, model, device, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for data in loader:  # Use provided loader instead of fixed val_loader\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data.x, data.edge_index)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return correct / total, f1\n",
    "\n",
    "\n",
    "\n",
    "def validation_loss(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for data in val_loader:\n",
    "        data, target = data.to(device), data.y.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data.x, data.edge_index)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(val_loader)\n",
    "    return average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def generate_graphs(folder='results'):\n",
    "    graphs = []\n",
    "    \n",
    "    for i, file in enumerate(os.listdir(folder)):\n",
    "        data_path = os.path.join(folder, file)\n",
    "\n",
    "        # Check file extension and handle accordingly\n",
    "        file_ext = os.path.splitext(file)[1]\n",
    "        \n",
    "        if file_ext == \".csv\":\n",
    "            try:\n",
    "                df = pd.read_csv(data_path)\n",
    "                # Your graph generation code here...\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    df = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "                    # Your graph generation code here...\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read {data_path} due to {str(e)}\")\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"{data_path} is not a valid CSV or is empty.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {str(e)}\")\n",
    "            \n",
    "        elif file_ext == \".pt\":\n",
    "            try:\n",
    "                # Load your PyTorch tensor or model\n",
    "                data = torch.load(data_path)\n",
    "                # Your code to handle PyTorch data here...\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load PyTorch file {data_path} due to {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"Unsupported file type {file_ext} for file {data_path}\")\n",
    "    return graphs\n",
    "\n",
    "# Usage\n",
    "graphs = generate_graphs('/Users/mac/Desktop/gnn/PFE_Feryal/results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4867bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch Size: 100, Train Graphs Length: {len(train_graphs)}, Validation Graphs Length: {len(val_graphs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ba294",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GNN(in_channels=64, hidden_channels=32, num_classes=10).to(device)  # Adjust parameters as needed\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Or your specific loss\n",
    "train_size = int(0.8 * len(graphs))\n",
    "val_size = len(graphs) - train_size\n",
    "train_graphs, val_graphs = random_split(graphs, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=100, shuffle=True, collate_fn=Batch.from_data_list)\n",
    "val_loader = DataLoader(val_graphs, batch_size=100, shuffle=False, collate_fn=Batch.from_data_list)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Your data loading and splitting logic here\n",
    "num_epochs = 100\n",
    "train_loss_history, val_loss_history = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "    val_loss = validate(val_loader, model, device, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "def generate_graphs(folder='results'):\n",
    "    \"\"\"Generate graphs one by one from saved chunks.\"\"\"\n",
    "    saved_files = [f for f in os.listdir(folder) if f.startswith('result_batch_')]\n",
    "    for file_name in saved_files:\n",
    "        yield torch.load(os.path.join(folder, file_name))\n",
    "\n",
    "# Hyperparameters & Optimizers\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-5\n",
    "hidden_channels = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 4\n",
    "num_classes = df['object_pose'].nunique()  # Ensure df is defined\n",
    "\n",
    "# Model\n",
    "model = GNN(input_dim, hidden_channels, num_classes).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Criterion/Loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Loop through all graphs\n",
    "\n",
    "for graph in generate_graphs():\n",
    "    # Ensure graph is in the right format for your GNN and create a Batch.\n",
    "    data = Batch.from_data_list([graph]).to(device)\n",
    "\n",
    "    # Perform a forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data.x, data.edge_index)\n",
    "\n",
    "        # Calculating softmax probabilities and getting predicted classes\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted_classes = torch.max(probabilities, 1)\n",
    "\n",
    "    # Displaying results\n",
    "    print(\"Outputs:\", outputs)\n",
    "    print(\"Probabilities:\", probabilities)\n",
    "    print(\"Predicted classes:\", predicted_classes)\n",
    "\n",
    "    # Optionally: you might want to clear memory between graphs\n",
    "    del graph, data, outputs, probabilities, predicted_classes\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d63f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have df, training_data, val_data\n",
    "num_classes = df['object_pose'].nunique()  # Ensure df is defined\n",
    "input_dim = 4\n",
    "hidden_channels = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model initialization, Criterion, and Optimizer definition remains unchanged.\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training and Validation Functions\n",
    "def train_epoch(loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0  \n",
    "    total = 0\n",
    "    for data in loader:  # Use provided loader instead of fixed train_loader\n",
    "        data, target = data.to(device), data.y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.x, data.edge_index)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    average_loss = total_loss / len(loader)\n",
    "    train_accuracy = correct / total  \n",
    "    val_accuracy , val_f1 = validate(val_loader, model, device, criterion)  # Ensure validate function is defined with proper parameters\n",
    "    \n",
    "    return average_loss, train_accuracy\n",
    "\n",
    "def validate(loader, model, device, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for data in loader:  # Use provided loader instead of fixed val_loader\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data.x, data.edge_index)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return correct / total, f1\n",
    "\n",
    "\n",
    "# Epoch loop\n",
    "num_epochs = 20 \n",
    "for epoch in range(num_epochs):\n",
    "    train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "    validate(val_loader, model, device, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2917468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "train_temporal_loader = DataLoader(train_temporal_data, batch_size=32, shuffle=True)\n",
    "val_temporal_loader = DataLoader(val_temporal_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "def run_epoch(epoch, loader, model, optimizer, criterion, device, mode=\"Spatial\"):\n",
    "    train_loss, train_accuracy = train_epoch(loader, model, optimizer, criterion, device)\n",
    "    val_loss = validation_loss(val_loader, model, criterion, device)\n",
    "    val_accuracy, val_f1 = validate(val_loader, model, device, criterion)\n",
    "    \n",
    "    print(f\"[{mode}] Epoch {epoch + 1}, Train Loss: {train_loss:.9f}, Train Accuracy: {train_accuracy:.9f}, Val Accuracy: {val_accuracy:.9f}, Validation F1 Score: {val_f1:.9f}\")\n",
    "\n",
    "    return train_accuracy, val_accuracy, val_f1\n",
    "\n",
    "num_epochs = 20 \n",
    "\n",
    "spatial_accuracies = []\n",
    "train_accuracies = []\n",
    "spatial_f1_scores = []\n",
    "\n",
    "temp_train_accuracies = []\n",
    "temporal_accuracies = []\n",
    "temporal_f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc, val_acc, val_f1 = run_epoch(epoch, train_loader, model, optimizer, criterion_class, device, \"Spatial\")\n",
    "    spatial_accuracies.append(val_acc)\n",
    "    train_accuracies.append(train_acc)\n",
    "    spatial_f1_scores.append(val_f1)\n",
    "    \n",
    "    # Assuming train_temporal_loader and val_temporal_loader are defined properly\n",
    "    temp_train_acc, temp_val_acc, temp_val_f1 = run_epoch(epoch, train_temporal_loader, model, optimizer, criterion_class, device, \"Temporal\")\n",
    "    temporal_accuracies.append(temp_val_acc)\n",
    "    temp_train_accuracies.append(temp_train_acc)\n",
    "    temporal_f1_scores.append(temp_val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b466d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_frames = df['frame_num'].unique()\n",
    "graphs = [construct_graph_for_frame(df[df['frame_num'] == frame]) for frame in unique_frames]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming model, optimizer, criterion are defined elsewhere\n",
    "model = model.to(device)\n",
    "\n",
    "criterion_class = nn.CrossEntropyLoss()  # For classification\n",
    "criterion_bbox = nn.MSELoss()  # For bounding box regression, assuming you want to use Mean Square Error. \n",
    "# For spatial graphs:\n",
    "\n",
    "spatial_accuracies = []\n",
    "train_accuracies = []  # List to hold training accuracies\n",
    "spatial_f1_scores = []\n",
    "\n",
    "val_loss_history = []\n",
    "\n",
    "best_val_loss = float('inf')  # Set to a very high value for initialization\n",
    "counter = 0\n",
    "patience =5  # Assuming a patience of 5; \n",
    "\n",
    "num_epochs = 20 # Example: Ensure you define this parameter\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "    val_loss = validation_loss(val_loader, model, criterion, device)  # Ensure this function is defined/used consistently\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_accuracy = validate(val_loader, model, device, criterion)\n",
    "    spatial_accuracies.append(val_accuracy)\n",
    "    train_accuracies.append(train_accuracy)  # Append the training accuracy\n",
    "    \n",
    "    val_accuracy, val_f1 = validate(val_loader, model, device, criterion)\n",
    "    spatial_f1_scores.append(val_f1)  # Storing the F1 score for visualization\n",
    "\n",
    "    \n",
    "    print(f\"[Spatial] Epoch {epoch + 1}, Train Loss: {train_loss:.9f}, Train Accuracy: {train_accuracy:.9f}, Val Accuracy: {val_accuracy:.9f} , Validation F1 Score: {val_f1:.9f}\")\n",
    "    \n",
    "# For temporal graphs:\n",
    "temp_train_accuracies = [] \n",
    "temporal_graphs = []\n",
    "temporal_f1_scores = []\n",
    "for i in range(len(unique_frame_numbers) - 1):\n",
    "    current_frame = df[df['frame_num'] == unique_frame_numbers[i]]\n",
    "    next_frame = df[df['frame_num'] == unique_frame_numbers[i+1]]\n",
    "    graph = construct_temporal_graph_for_frame(current_frame, next_frame)\n",
    "    temporal_graphs.append(graph)\n",
    "\n",
    "\n",
    "    \n",
    "train_temporal_loader = DataLoader(train_temporal_graphs, batch_size=32, shuffle=True)\n",
    "val_temporal_loader = DataLoader(val_temporal_graphs, batch_size=32, shuffle=False)\n",
    "temporal_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = train_epoch(train_loader, model, optimizer, criterion_class, device)\n",
    "\n",
    "   \n",
    "    \n",
    "    temporal_accuracies.append(val_accuracy)\n",
    "    temp_train_accuracies.append(train_accuracy)  # Append the training accuracy\n",
    "    val_accuracy, val_f1 = validate(val_loader, model, device, criterion)\n",
    "    temporal_f1_scores.append(val_f1)  # Storing the F1 score for visualization\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"[Temporal] Epoch {epoch + 1}, Train Loss: {train_loss:.9f}, Train Accuracy: {train_accuracy:.9f}, Val Accuracy: {val_accuracy:.9f},Validation F1 Score: {val_f1:.9f}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracies, '--', label=\"Spatial Training Accuracy\")\n",
    "plt.plot(spatial_accuracies, label=\"Spatial Validation Accuracy\")\n",
    "plt.plot(temp_train_accuracies, '--', label=\"Temporal Training Accuracy\")\n",
    "plt.plot(temporal_accuracies, label=\"Temporal Validation Accuracy\")\n",
    "plt.plot(spatial_f1_scores, '-.', label=\"Spatial Validation F1 Score\")  # New line for F1 score visualization\n",
    "plt.plot(temporal_f1_scores, ':', label=\"Temporal Validation F1 Score\")  # New line for F1 score visualization\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.title('Performance Comparison')\n",
    "plt.savefig('Performance Comparison spa vs temp ALLdatasetgat.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1938bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_save_path = \"/Users/mac/Desktop/gnn/archive/Performance Comparison.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.1, verbose=True)\n",
    "\n",
    "# Criterion/Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Visualization Lists\n",
    "train_accuracies, spatial_accuracies, spatial_f1_scores = [], [], []\n",
    "temp_train_accuracies, temporal_accuracies, temporal_f1_scores = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader Definitions\n",
    "train_spatial_loader = DataLoader(train_spatial_dataset, batch_size=32, shuffle=True)\n",
    "val_spatial_loader = DataLoader(val_spatial_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_temporal_loader = DataLoader(train_temporal_dataset, batch_size=32, shuffle=True)\n",
    "val_temporal_loader = DataLoader(val_temporal_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Early stopping initialization\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training loop for Spatial Graphs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "    # Validation\n",
    "    val_loss = validation_loss(val_loader, model, criterion, device)  \n",
    "    val_accuracy, val_f1 = validate(val_loader, model, device, criterion)\n",
    "    \n",
    "    # Visualization Data\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    spatial_accuracies.append(val_accuracy)\n",
    "    spatial_f1_scores.append(val_f1)  \n",
    "    \n",
    "    # Logging\n",
    "    print(f\"[Spatial] Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Scheduler and Early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_spatial_model_weights.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered for spatial training.\")\n",
    "            break\n",
    "\n",
    "# Ensure temporal graphs are prepared and loaders (train_temporal_loader and val_temporal_loader) are defined\n",
    "\n",
    "# Resetting early stopping and best_val_loss\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training loop for Temporal Graphs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(train_temporal_loader, model, optimizer, criterion, device)\n",
    "    # Validation\n",
    "    val_loss = validation_loss(val_temporal_loader, model, criterion, device)  \n",
    "    val_accuracy, val_f1 = validate(val_temporal_loader, model, device, criterion)\n",
    "    \n",
    "    # Visualization Data\n",
    "    temp_train_accuracies.append(train_accuracy)\n",
    "    temporal_accuracies.append(val_accuracy)\n",
    "    temporal_f1_scores.append(val_f1)  \n",
    "    \n",
    "    # Logging\n",
    "    print(f\"[Temporal] Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Scheduler and Early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_temporal_model_weights.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered for temporal training.\")\n",
    "            break\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracies, '--', label=\"Spatial Training Accuracy\")\n",
    "plt.plot(spatial_accuracies, label=\"Spatial Validation Accuracy\")\n",
    "plt.plot(temp_train_accuracies, '--', label=\"Temporal Training Accuracy\")\n",
    "plt.plot(temporal_accuracies, label=\"Temporal Validation Accuracy\")\n",
    "plt.plot(spatial_f1_scores, '-.', label=\"Spatial Validation F1 Score\")\n",
    "plt.plot(temporal_f1_scores, ':', label=\"Temporal Validation F1 Score\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.title('Performance Comparison: Spatial vs Temporal')\n",
    "plt.savefig('Performance_Comparison_spa_vs_temp_ALLdatasetgat.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'construct_graph_for_frame' and 'process_frame' functions are defined as per your previous codes\n",
    "\n",
    "def save_result_to_disk(result, batch_index, folder='results'):\n",
    "    \"\"\"Save the result object to disk using torch.save.\"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    file_path = os.path.join(folder, f'result_batch_{batch_index}.pt')\n",
    "    torch.save(result, file_path)\n",
    "\n",
    "def process_frame(frame_num, df_frame):\n",
    "    return frame_num, construct_graph_for_frame(df_frame)\n",
    "\n",
    "def process_and_save(df, chunk_size):\n",
    "    \"\"\"Process data in chunks and save each chunk's result to disk.\"\"\"\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Extract the chunk of data\n",
    "        chunk_data = df.iloc[i*chunk_size : (i+1)*chunk_size]\n",
    "        \n",
    "        # Process the chunk\n",
    "        frame_num, result = process_frame(i, chunk_data)\n",
    "        \n",
    "        # Save the result to disk\n",
    "        save_result_to_disk(result, frame_num)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame\n",
    "# Note: Please ensure df is defined in your actual use-case before running this code snippet\n",
    "chunk_size = 1000\n",
    "process_and_save(df, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c9d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
